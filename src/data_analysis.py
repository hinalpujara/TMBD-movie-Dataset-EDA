# -*- coding: utf-8 -*-
"""Data_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1szkzDsRR5P1wlJsxZY7MshZHc-wTg9EA

# Midterm Project
**Team Members**

1) Hinal Pujara

2) Sravni

# Data Explaretion
"""

import pandas as pd
import io

# Reading the file, handling encoding errors
try:
    with open("/content/TMDB_movie_dataset_v11.csv", 'r', encoding='utf-8') as file:
        content = file.read()
except UnicodeDecodeError:
    with open("/content/TMDB_movie_dataset_v11.csv", 'r', encoding='latin-1') as file:
        content = file.read()

# Replace potential problematic line terminators
content = content.replace('\r\n', '\n')
content = content.replace('\r', '\n')

# Read the data using io.StringIO and specifying error handling
data = pd.read_csv(
    io.StringIO(content),
    on_bad_lines='skip',
    engine='python',
    sep=',',
    quotechar='"',
    encoding='utf-8'
)
data.head()

data.shape

data.isnull().sum()

data.duplicated().sum()

data.info()

"""# Data Clening

## Remove unnecessary columns
"""

columns_to_drop = [
    'adult',
    'backdrop_path',
    'poster_path',
    'homepage',
    'imdb_id',
    'original_language',
    'original_title'
]

cleaned_df = data.drop(columns=columns_to_drop)
print("Columns after removal:", cleaned_df.columns.tolist())
print("New shape:", cleaned_df.shape)

# Handle movies with zero runtime
print("Before runtime cleaning:", len(cleaned_df))
median_runtime = cleaned_df[cleaned_df['runtime'] > 0]['runtime'].median()
cleaned_df['runtime'] = cleaned_df['runtime'].replace(0, median_runtime)

# Remove extreme runtime outliers (e.g., 14400 minutes = 10 days)
cleaned_df = cleaned_df[cleaned_df['runtime'] <= 300]  # Remove movies longer than 5 hours
print("After runtime cleaning:", len(cleaned_df))

# Handle vote averages
# Remove movies with no votes as they don't have meaningful ratings
cleaned_df = cleaned_df[cleaned_df['vote_count'] > 0]
print("After vote cleaning:", len(cleaned_df))

print("\nRuntime statistics after cleaning:")
print(cleaned_df['runtime'].describe())

"""## Handle missing values"""

# Drop row with missing value
data.dropna(inplace=True)

import numpy as np

# Verify the quality of our cleaned data
def check_data_quality(df):
    """
    Perform quality checks on the cleaned dataset
    """
    print("Data Quality Report:")
    print("-" * 50)

    # 1. Check for any remaining nulls
    print("\n1. Null Values Check:")
    null_counts = df.isnull().sum()
    print(null_counts[null_counts > 0] if len(null_counts[null_counts > 0]) > 0 else "No null values found")

    # 2. Check value ranges for numeric columns
    print("\n2. Numeric Columns Summary:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    print(df[numeric_cols].describe())

    # 3. Check unique values in categorical columns
    print("\n3. Categorical Columns Sample Counts:")
    categorical_cols = ['status', 'genres', 'production_companies', 'production_countries']
    for col in categorical_cols:
        print(f"\n{col} - Unique values count: {df[col].nunique()}")
        print("Sample values:", df[col].value_counts().head(3))

    # 4. Check for any suspicious values
    print("\n4. Suspicious Values Check:")
    print("Negative revenues:", len(df[df['revenue'] < 0]))
    print("Negative budgets:", len(df[df['budget'] < 0]))
    print("Zero runtimes:", len(df[df['runtime'] == 0]))

    return df

# Run quality check
cleaned_df = check_data_quality(cleaned_df)

"""## Remove duplicates and standardize dates"""

initial_rows = len(cleaned_df)
cleaned_df = cleaned_df.drop_duplicates()
dropped_rows = initial_rows - len(cleaned_df)

print(f"Removed {dropped_rows} duplicate rows")
print(f"New shape: {cleaned_df.shape}")

""" ## Create derived features"""

cleaned_df['release_date'] = pd.to_datetime(cleaned_df['release_date'], errors='coerce')
cleaned_df = cleaned_df.dropna(subset=['release_date'])

# Create date-related features
cleaned_df['release_year'] = cleaned_df['release_date'].dt.year
cleaned_df['release_month'] = cleaned_df['release_date'].dt.month

print("Sample of date features:")
print(cleaned_df[['release_date', 'release_year', 'release_month']].head())

"""## Calculate ROI"""

cleaned_df['roi'] = np.where(
    cleaned_df['budget'] > 0,
    (cleaned_df['revenue'] - cleaned_df['budget']) / cleaned_df['budget'],
    0
)

# Final dataset overview
print("Final Dataset Info:")
print(cleaned_df.info())
print("\nSample of final cleaned data:")
print(cleaned_df.head())

import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("darkgrid")


sns.set_palette("husl")

"""# Check for Outliers using Box Plots and Histograms"""

def plot_outliers(df, columns):
    """
    Create box plots and histograms for specified columns
    """
    for column in columns:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # Box Plot
        sns.boxplot(x=df[column], ax=ax1)
        ax1.set_title(f'Box Plot of {column}')

        # Histogram
        sns.histplot(data=df, x=column, ax=ax2)
        ax2.set_title(f'Distribution of {column}')

        plt.tight_layout()
        plt.show()

# Plot outliers for numeric columns
numeric_columns = ['runtime', 'vote_average', 'popularity', 'budget', 'revenue']
plot_outliers(cleaned_df, numeric_columns)

"""# Create Pair Plot for Feature Relationships"""

# Select relevant numeric columns for pair plot
pair_columns = ['vote_average', 'popularity', 'runtime', 'revenue', 'budget']
sns.pairplot(cleaned_df[pair_columns], diag_kind='kde')
plt.suptitle('Pair Plot of Movie Features', y=1.02)
plt.show()

"""#  Correlation Heatmap"""

def create_correlation_heatmap(df):
    """
    Create a correlation heatmap for numeric features
    """
    # Select numeric columns
    numeric_df = df.select_dtypes(include=[np.number])

    # Calculate correlation matrix
    corr_matrix = numeric_df.corr()

    # Create heatmap
    plt.figure(figsize=(12, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation Heatmap of Movie Features')
    plt.tight_layout()
    plt.show()

create_correlation_heatmap(cleaned_df)

"""# Scatter Plot with Regression Line"""

def plot_scatter_with_reg(df, x_col, y_col):
    """
    Create scatter plot with regression line
    """
    plt.figure(figsize=(10, 6))
    sns.regplot(data=df, x=x_col, y=y_col)
    plt.title(f'{x_col} vs {y_col}')
    plt.show()

# Plot budget vs revenue
plot_scatter_with_reg(cleaned_df, 'budget', 'revenue')

"""# Data Slicing and Filtering"""

class MovieAnalyzer:
    """
    Class for analyzing movie data with various filtering capabilities
    """
    def __init__(self, df):
        self.df = df

    def filter_by_year(self, year):
        """Filter movies by release year"""
        return self.df[self.df['release_year'] == year]

    def filter_by_genre(self, genre):
        """Filter movies by genre"""
        return self.df[self.df['genres'].str.contains(genre, case=False, na=False)]

    def get_top_movies(self, column, n=10, ascending=False):
        """Get top N movies based on a specific column"""
        return self.df.nlargest(n, column)

    def get_movies_by_condition(self, column, condition, value):
        """
        Filter movies based on a condition
        Example: get_movies_by_condition('budget', '>', 1000000)
        """
        if condition == '>':
            return self.df[self.df[column] > value]
        elif condition == '<':
            return self.df[self.df[column] < value]
        elif condition == '==':
            return self.df[self.df[column] == value]

    def get_statistical_summary(self, *columns):
        """Get statistical summary of specified columns"""
        return self.df[list(columns)].describe()

# Create analyzer instance
analyzer = MovieAnalyzer(cleaned_df)

print("Movies from 2020:")
analyzer.filter_by_year(2020).head()

print("\nTop 5 highest budget movies:")
analyzer.get_top_movies('budget', 5)[['title', 'budget', 'revenue']]

print("\nStatistical summary of revenue and budget:")
analyzer.get_statistical_summary('revenue', 'budget')

"""# NumPy Array Operations"""

def perform_numpy_operations(*args, **kwargs):
    """
    Demonstrate NumPy operations on movie data
    """
    try:
        # Convert relevant columns to NumPy arrays
        revenue_array = args[0]['revenue'].to_numpy()
        budget_array = args[0]['budget'].to_numpy()

        # Basic statistics using NumPy
        stats = {
            'mean_revenue': np.mean(revenue_array),
            'median_budget': np.median(budget_array),
            'std_revenue': np.std(revenue_array),
            'revenue_budget_ratio': np.mean(revenue_array/np.where(budget_array == 0, 1, budget_array))
        }

        # Additional calculations based on kwargs
        if 'percentile' in kwargs:
            stats['revenue_percentile'] = np.percentile(revenue_array, kwargs['percentile'])

        return stats

    except Exception as e:
        print(f"Error in numpy operations: {e}")
        return None


stats = perform_numpy_operations(cleaned_df, percentile=90)
print("NumPy Statistical Analysis:")
for key, value in stats.items():
    print(f"{key}: {value:,.2f}")